{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayoubelhariri/hala/blob/main/CollabWORK_Mapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INTAKE:** CHECK THE FILE"
      ],
      "metadata": {
        "id": "E8UGC80HJ6Ah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This script defines functions to identify appropriate parsers for files/URLs and to process inputs,\n",
        "and then checks if the file is compressed and unzips it.\n",
        "\"\"\"\n",
        "# Added a pass statement to avoid 'incomplete input' SyntaxError\n",
        "pass\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import urllib.parse\n",
        "import requests\n",
        "import sys # Import sys to access command line arguments\n",
        "import zipfile # Added for unzip functionality\n",
        "import tarfile # Added for unzip functionality\n",
        "import gzip # Added for unzip functionality\n",
        "import shutil # Added for unzip functionality\n",
        "\n",
        "\n",
        "def get_file_type(file_path):\n",
        "    \"\"\"Identifies the file type for parsing or unzipping.\"\"\"\n",
        "    # Handle complex extensions like .tar.gz\n",
        "    if file_path.lower().endswith('.tar.gz'):\n",
        "        return '.tar.gz'\n",
        "    _, extension = os.path.splitext(file_path)\n",
        "    return extension.lower()\n",
        "\n",
        "def unzip_file_if_needed(file_path, extract_to_dir='.'):\n",
        "    \"\"\"\n",
        "    Checks if a file is a compressed archive and extracts it if it is.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the file to check and potentially extract.\n",
        "        extract_to_dir (str): The directory where the contents should be extracted.\n",
        "                              Defaults to the current directory.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "        return\n",
        "\n",
        "    file_type = get_file_type(file_path)\n",
        "    archive_types = ['.zip', '.tar', '.gz', '.tar.gz', '.tgz']\n",
        "\n",
        "    if file_type not in archive_types:\n",
        "        print(f\"'{os.path.basename(file_path)}' is not a recognized compressed file. No action taken.\")\n",
        "        return None # Not an archive\n",
        "\n",
        "    file_name = os.path.basename(file_path)\n",
        "    print(f\"Archive detected: '{file_name}'. Extracting...\")\n",
        "\n",
        "    # Create a unique extraction folder name from the archive file name\n",
        "    extraction_folder_name = file_name.replace('.tar.gz', '').replace('.zip', '').replace('.tgz', '').replace('.gz', '').replace('.tar', '')\n",
        "    extraction_path = os.path.join(extract_to_dir, extraction_folder_name)\n",
        "\n",
        "    if not os.path.exists(extraction_path):\n",
        "        os.makedirs(extraction_path)\n",
        "\n",
        "    try:\n",
        "        if file_type == '.zip':\n",
        "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extraction_path)\n",
        "        elif file_type in ['.tar', '.tar.gz', '.tgz']:\n",
        "            with tarfile.open(file_path, 'r:*') as tar_ref:\n",
        "                tar_ref.extractall(path=extraction_path)\n",
        "        elif file_type == '.gz':\n",
        "            output_filename = os.path.join(extraction_path, os.path.splitext(file_name)[0])\n",
        "            with gzip.open(file_path, 'rb') as f_in:\n",
        "                with open(output_filename, 'wb') as f_out:\n",
        "                    shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "        print(f\"Successfully extracted to '{extraction_path}'\")\n",
        "        return extraction_path\n",
        "    except (zipfile.BadZipFile, tarfile.ReadError, IOError) as e:\n",
        "        print(f\"Error during extraction of '{file_name}': {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_parser_for_file(file_path):\n",
        "    \"\"\"\n",
        "    Identifies the appropriate Python parser/library for a given file path or URL path.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The full path to the file or the path component of a URL.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the parser name (str) and the file extension (str).\n",
        "               Returns (None, None) if the input is invalid.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get the file extension from the path. The second element of the splitext result is the extension.\n",
        "        # We use lower() to make the comparison case-insensitive (e.g., .JPG vs .jpg).\n",
        "        _, file_extension = os.path.splitext(file_path)\n",
        "        file_extension = file_extension.lower()\n",
        "    except (TypeError, AttributeError):\n",
        "        return None, None\n",
        "\n",
        "    # A dictionary mapping common file extensions to their recommended Python parsers/libraries.\n",
        "    parser_mapping = {\n",
        "        # Data Formats\n",
        "        '.csv': 'csv library (built-in)',\n",
        "        '.json': 'json library (built-in)',\n",
        "        '.xml': 'xml.etree.ElementTree (built-in) or lxml library',\n",
        "        '.yaml': 'PyYAML library',\n",
        "        '.yml': 'PyYAML library',\n",
        "        '.ini': 'configparser library (built-in)',\n",
        "        '.conf': 'configparser library (built-in)',\n",
        "        '.xls': 'xlrd library or pandas library',\n",
        "        '.xlsx': 'openpyxl library or pandas library',\n",
        "\n",
        "        # Document Formats\n",
        "        '.txt': 'Standard file I/O (open() function)',\n",
        "        '.md': 'markdown library or mistune library',\n",
        "        '.pdf': 'PyPDF2 library or pdfplumber library',\n",
        "        '.docx': 'python-docx library',\n",
        "        '.doc': 'python-docx library (may have limitations with older .doc formats)',\n",
        "        '.rtf': 'striprtf library',\n",
        "\n",
        "        # Image Formats (for metadata or processing)\n",
        "        '.jpg': 'Pillow (PIL) library',\n",
        "        '.jpeg': 'Pillow (PIL) library',\n",
        "        '.png': 'Pillow (PIL) library',\n",
        "        '.gif': 'Pillow (PIL) library',\n",
        "        '.bmp': 'Pillow (PIL) library',\n",
        "        '.tiff': 'Pillow (PIL) library',\n",
        "\n",
        "        # Compressed Files\n",
        "        '.zip': 'zipfile library (built-in)',\n",
        "        '.tar': 'tarfile library (built-in)',\n",
        "        '.gz': 'gzip library (built-in)',\n",
        "        '.tar.gz': 'tarfile library (built-in)', # Added for consistency with unzip\n",
        "        '.tgz': 'tarfile library (built-in)',    # Added for consistency with unzip\n",
        "    }\n",
        "\n",
        "    # Look up the extension in the mapping.\n",
        "    parser = parser_mapping.get(file_extension, f\"Unsupported file type\")\n",
        "    return parser, file_extension\n",
        "\n",
        "def process_input(input_path):\n",
        "    \"\"\"\n",
        "    Processes the input, whether it's a local file path or a URL,\n",
        "    and then checks and unzips the file if needed.\n",
        "    \"\"\"\n",
        "    downloaded_file_path = None # To store the path of the downloaded file if it's a URL\n",
        "\n",
        "    # Check if the input is a URL\n",
        "    if input_path.startswith('http://') or input_path.startswith('https://'):\n",
        "        print(f\"URL detected: {input_path}\")\n",
        "        parsed_url = urllib.parse.urlparse(input_path)\n",
        "        path = parsed_url.path\n",
        "        filename = os.path.basename(path)\n",
        "        downloaded_file_path = filename # Assuming download saves to current dir\n",
        "\n",
        "        parser, extension = get_parser_for_file(path)\n",
        "        print(f\"-> File '{filename}' needs parser: {parser}\")\n",
        "\n",
        "        # Attempt to download the file\n",
        "        print(f\"-> Attempting to download '{filename}'...\")\n",
        "        try:\n",
        "            response = requests.get(input_path, stream=True)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            with open(filename, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "            print(f\"-> Successfully downloaded and saved as '{filename}' in the current directory.\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"-> Error: Could not download the file. {e}\")\n",
        "            return # Exit if download fails\n",
        "\n",
        "    else:\n",
        "        # Treat as a local file path\n",
        "        print(f\"Local file path detected: {input_path}\")\n",
        "        filename = os.path.basename(input_path)\n",
        "        downloaded_file_path = input_path # Use the provided path for local files\n",
        "\n",
        "        parser, extension = get_parser_for_file(input_path)\n",
        "        print(f\"-> File '{filename}' needs parser: {parser}\")\n",
        "\n",
        "    # --- Integrate the UNZIP step here ---\n",
        "    if downloaded_file_path and os.path.exists(downloaded_file_path):\n",
        "        print(\"\\n--- Checking for compression ---\")\n",
        "        unzip_file_if_needed(downloaded_file_path, extract_to_dir='.') # Extract to current directory or specify\n",
        "        # Note: unzip_file_if_needed returns the extraction path or None.\n",
        "        # You might want to capture this return value for the next steps."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "n7mqLs7a82Zz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONDITIONAL:** UNZIP IF FILE TYPE IS COMPRESSED."
      ],
      "metadata": {
        "id": "C6kOsrBD-s5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tarfile\n",
        "import gzip\n",
        "import shutil\n",
        "\n",
        "# To make this script runnable, we'll include the function from INTAKE.py.\n",
        "# In a real project, you would use: from INTAKE import get_parser_for_file\n",
        "def get_parser_for_file(file_path):\n",
        "    \"\"\"\n",
        "    Identifies the appropriate Python parser/library for a given file based on its extension.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Handle complex extensions like .tar.gz\n",
        "        if file_path.lower().endswith('.tar.gz'):\n",
        "            file_extension = '.tar.gz'\n",
        "        else:\n",
        "            _, file_extension = os.path.splitext(file_path)\n",
        "            file_extension = file_extension.lower()\n",
        "    except (TypeError, AttributeError):\n",
        "        return \"Invalid file path provided.\"\n",
        "\n",
        "    parser_mapping = {\n",
        "        '.zip': 'zipfile library (built-in)',\n",
        "        '.tar': 'tarfile library (built-in)',\n",
        "        '.gz': 'gzip library (built-in)',\n",
        "        '.tar.gz': 'tarfile library (built-in)',\n",
        "        '.tgz': 'tarfile library (built-in)',  # Added .tgz support\n",
        "    }\n",
        "    parser = parser_mapping.get(file_extension, \"Not a standard compressed file.\")\n",
        "    return parser\n",
        "\n",
        "def unzip_file_if_needed(file_path, extract_to_dir='.'):\n",
        "    \"\"\"\n",
        "    Checks if a file is a compressed archive and extracts it if it is.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the file to check and potentially extract.\n",
        "        extract_to_dir (str): The directory where the contents should be extracted.\n",
        "                              Defaults to the current directory.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Error: File '{file_path}' not found.\")\n",
        "        return\n",
        "\n",
        "    parser_info = get_parser_for_file(file_path)\n",
        "    file_name = os.path.basename(file_path)\n",
        "    # Create a destination directory name from the archive file name\n",
        "    extraction_folder_name = file_name.replace('.tar.gz', '').replace('.zip', '').replace('.tgz', '').replace('.gz', '')\n",
        "    extraction_folder = os.path.join(extract_to_dir, extraction_folder_name)\n",
        "\n",
        "\n",
        "    try:\n",
        "        if 'zipfile' in parser_info:\n",
        "            print(f\"'{file_name}' is a zip file. Extracting...\")\n",
        "            if not os.path.exists(extraction_folder):\n",
        "                os.makedirs(extraction_folder)\n",
        "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extraction_folder)\n",
        "            print(f\"Successfully extracted to '{extraction_folder}'\")\n",
        "\n",
        "        elif 'tarfile' in parser_info:\n",
        "            print(f\"'{file_name}' is a tar archive. Extracting...\")\n",
        "            if not os.path.exists(extraction_folder):\n",
        "                os.makedirs(extraction_folder)\n",
        "            # 'r:*' automatically handles compression like gzip\n",
        "            with tarfile.open(file_path, 'r:*') as tar_ref:\n",
        "                tar_ref.extractall(path=extraction_folder)\n",
        "            print(f\"Successfully extracted to '{extraction_folder}'\")\n",
        "\n",
        "        elif 'gzip' in parser_info:\n",
        "            print(f\"'{file_name}' is a gzip file. Decompressing...\")\n",
        "            # Gzip typically compresses a single file, so we extract it directly\n",
        "            output_filename = os.path.join(extract_to_dir, os.path.splitext(file_name)[0])\n",
        "            if not os.path.exists(extract_to_dir):\n",
        "                os.makedirs(extract_to_dir)\n",
        "            with gzip.open(file_path, 'rb') as f_in:\n",
        "                with open(output_filename, 'wb') as f_out:\n",
        "                    shutil.copyfileobj(f_in, f_out)\n",
        "            print(f\"Successfully decompressed to '{output_filename}'\")\n",
        "\n",
        "        else:\n",
        "            print(f\"'{file_name}' is not a recognized compressed file. No action taken.\")\n",
        "\n",
        "    except (zipfile.BadZipFile, tarfile.ReadError, IOError) as e:\n",
        "        print(f\"An error occurred during extraction of '{file_name}': {e}\")\n",
        "\n",
        "\n",
        "# --- Main execution block to demonstrate the function ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- Setting up test files ---\")\n",
        "\n",
        "    # Create a dummy directory for our test files\n",
        "    test_dir = \"test_archives\"\n",
        "    if not os.path.exists(test_dir):\n",
        "        os.makedirs(test_dir)\n",
        "\n",
        "    # Create a dummy file to be compressed\n",
        "    dummy_file_path = os.path.join(test_dir, \"sample_text.txt\")\n",
        "    with open(dummy_file_path, \"w\") as f:\n",
        "        f.write(\"This is a test file for the unzipping script.\\n\")\n",
        "        f.write(\"It will be placed inside various archives.\\n\")\n",
        "\n",
        "    # 1. Create a dummy ZIP file\n",
        "    zip_path = os.path.join(test_dir, \"my_archive.zip\")\n",
        "    with zipfile.ZipFile(zip_path, 'w') as zf:\n",
        "        zf.write(dummy_file_path, os.path.basename(dummy_file_path))\n",
        "    print(f\"Created dummy zip: {zip_path}\")\n",
        "\n",
        "    # 2. Create a dummy TAR.GZ file\n",
        "    tar_path = os.path.join(test_dir, \"my_archive.tar.gz\")\n",
        "    with tarfile.open(tar_path, \"w:gz\") as tar:\n",
        "        tar.add(dummy_file_path, arcname=os.path.basename(dummy_file_path))\n",
        "    print(f\"Created dummy tar.gz: {tar_path}\")\n",
        "\n",
        "    # 3. Create a dummy GZ file\n",
        "    gz_path = os.path.join(test_dir, \"another_file.txt.gz\")\n",
        "    with open(dummy_file_path, 'rb') as f_in:\n",
        "        with gzip.open(gz_path, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "    print(f\"Created dummy gz: {gz_path}\")\n",
        "\n",
        "    # 4. Create a dummy TGZ file\n",
        "    tgz_path = os.path.join(test_dir, \"another_archive.tgz\")\n",
        "    with tarfile.open(tgz_path, \"w:gz\") as tar:\n",
        "        tar.add(dummy_file_path, arcname=os.path.basename(dummy_file_path))\n",
        "    print(f\"Created dummy tgz: {tgz_path}\")\n",
        "\n",
        "    # A non-compressed file for testing\n",
        "    non_zip_path = dummy_file_path\n",
        "\n",
        "    print(\"\\n--- Running UNZIP process ---\")\n",
        "\n",
        "    # List of files to process\n",
        "    files_to_process = [zip_path, tar_path, gz_path, tgz_path, non_zip_path]\n",
        "\n",
        "    for f in files_to_process:\n",
        "        unzip_file_if_needed(f, \"extracted_files\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "    # To see the results, the cleanup is commented out.\n",
        "    # You can manually delete the 'test_archives' and 'extracted_files' directories.\n",
        "    # shutil.rmtree(test_dir)\n",
        "    # shutil.rmtree(\"extracted_files\")\n",
        "    # print(\"\\n--- Cleaned up test files and directories ---\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pj97fqeC-i7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf0fa9a-b450-4b7c-8836-5034c2415d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Setting up test files ---\n",
            "Created dummy zip: test_archives/my_archive.zip\n",
            "Created dummy tar.gz: test_archives/my_archive.tar.gz\n",
            "Created dummy gz: test_archives/another_file.txt.gz\n",
            "Created dummy tgz: test_archives/another_archive.tgz\n",
            "\n",
            "--- Running UNZIP process ---\n",
            "'my_archive.zip' is a zip file. Extracting...\n",
            "Successfully extracted to 'extracted_files/my_archive'\n",
            "--------------------\n",
            "'my_archive.tar.gz' is a tar archive. Extracting...\n",
            "Successfully extracted to 'extracted_files/my_archive'\n",
            "--------------------\n",
            "'another_file.txt.gz' is a gzip file. Decompressing...\n",
            "Successfully decompressed to 'extracted_files/another_file.txt'\n",
            "--------------------\n",
            "'another_archive.tgz' is a tar archive. Extracting...\n",
            "Successfully extracted to 'extracted_files/another_archive'\n",
            "--------------------\n",
            "'sample_text.txt' is not a recognized compressed file. No action taken.\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PARSE** THE FILE CONTENT TO JSON."
      ],
      "metadata": {
        "id": "oPcOgejp_dcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "import configparser\n",
        "import xml.etree.ElementTree as ET\n",
        "import zipfile\n",
        "import tarfile\n",
        "import gzip\n",
        "import shutil\n",
        "\n",
        "# --- Functions from previous scripts (INTAKE.py and UNZIP.py) ---\n",
        "# In a real project, these would be imported from their respective files.\n",
        "\n",
        "def get_file_type(file_path):\n",
        "    \"\"\"Identifies the file type for parsing or unzipping.\"\"\"\n",
        "    if file_path.lower().endswith('.tar.gz'):\n",
        "        return '.tar.gz'\n",
        "    _, extension = os.path.splitext(file_path)\n",
        "    return extension.lower()\n",
        "\n",
        "def unzip_file_if_needed(file_path, extract_to_dir):\n",
        "    \"\"\"Checks if a file is a compressed archive and extracts it.\"\"\"\n",
        "    file_type = get_file_type(file_path)\n",
        "    archive_types = ['.zip', '.tar', '.gz', '.tar.gz', '.tgz']\n",
        "\n",
        "    if file_type not in archive_types:\n",
        "        return None # Not an archive\n",
        "\n",
        "    file_name = os.path.basename(file_path)\n",
        "    print(f\"Archive detected: '{file_name}'. Extracting...\")\n",
        "\n",
        "    # Create a unique extraction folder to avoid conflicts\n",
        "    extraction_folder_name = file_name.replace('.tar.gz', '').replace('.zip', '').replace('.tgz', '').replace('.gz', '').replace('.tar', '')\n",
        "    extraction_path = os.path.join(extract_to_dir, extraction_folder_name)\n",
        "\n",
        "    if not os.path.exists(extraction_path):\n",
        "        os.makedirs(extraction_path)\n",
        "\n",
        "    try:\n",
        "        if file_type == '.zip':\n",
        "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extraction_path)\n",
        "        elif file_type in ['.tar', '.tar.gz', '.tgz']:\n",
        "            with tarfile.open(file_path, 'r:*') as tar_ref:\n",
        "                tar_ref.extractall(path=extraction_path)\n",
        "        elif file_type == '.gz':\n",
        "            output_filename = os.path.join(extraction_path, os.path.splitext(file_name)[0])\n",
        "            with gzip.open(file_path, 'rb') as f_in:\n",
        "                with open(output_filename, 'wb') as f_out:\n",
        "                    shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "        print(f\"Successfully extracted to '{extraction_path}'\")\n",
        "        return extraction_path\n",
        "    except (zipfile.BadZipFile, tarfile.ReadError, IOError) as e:\n",
        "        print(f\"Error during extraction of '{file_name}': {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Core Parsing Logic ---\n",
        "\n",
        "def parse_file_to_json(file_path):\n",
        "    \"\"\"\n",
        "    Parses a single file (CSV, INI, XML, JSON) and returns its content as a Python dictionary.\n",
        "    \"\"\"\n",
        "    file_type = get_file_type(file_path)\n",
        "    file_name = os.path.basename(file_path)\n",
        "\n",
        "    print(f\"Parsing '{file_name}'...\")\n",
        "\n",
        "    try:\n",
        "        if file_type == '.csv':\n",
        "            with open(file_path, mode='r', encoding='utf-8') as csv_file:\n",
        "                # Use DictReader to automatically use the header row for keys\n",
        "                reader = csv.DictReader(csv_file)\n",
        "                return [row for row in reader]\n",
        "\n",
        "        elif file_type == '.ini':\n",
        "            config = configparser.ConfigParser()\n",
        "            config.read(file_path)\n",
        "            # Convert the config object to a nested dictionary\n",
        "            return {section: dict(config.items(section)) for section in config.sections()}\n",
        "\n",
        "        elif file_type == '.xml':\n",
        "            # A basic XML to dict converter. May not handle all complex cases (e.g., attributes).\n",
        "            def element_to_dict(element):\n",
        "                node = {}\n",
        "                for child in element:\n",
        "                    if child.tag not in node:\n",
        "                        node[child.tag] = element_to_dict(child) if len(child) > 0 else child.text\n",
        "                    else:\n",
        "                        # Handle multiple elements with the same tag\n",
        "                        if not isinstance(node[child.tag], list):\n",
        "                            node[child.tag] = [node[child.tag]]\n",
        "                        node[child.tag].append(element_to_dict(child) if len(child) > 0 else child.text)\n",
        "                return node\n",
        "\n",
        "            tree = ET.parse(file_path)\n",
        "            root = tree.getroot()\n",
        "            return {root.tag: element_to_dict(root)}\n",
        "\n",
        "        elif file_type == '.json':\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "\n",
        "        else:\n",
        "            print(f\"Unsupported file type for parsing: {file_type}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Could not parse '{file_name}'. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Main execution block ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Setup a test environment\n",
        "    base_dir = \"parsing_test\"\n",
        "    extracted_dir = os.path.join(base_dir, \"extracted\")\n",
        "    if os.path.exists(base_dir):\n",
        "        shutil.rmtree(base_dir)\n",
        "    os.makedirs(extracted_dir)\n",
        "\n",
        "    # 2. Create sample data files\n",
        "    # CSV\n",
        "    csv_path = os.path.join(base_dir, \"users.csv\")\n",
        "    with open(csv_path, \"w\", newline='') as f:\n",
        "        f.write(\"id,name,email\\n\")\n",
        "        f.write(\"1,Alice,alice@example.com\\n\")\n",
        "        f.write(\"2,Bob,bob@example.com\\n\")\n",
        "\n",
        "    # INI\n",
        "    ini_path = os.path.join(base_dir, \"settings.ini\")\n",
        "    with open(ini_path, \"w\") as f:\n",
        "        f.write(\"[database]\\n\")\n",
        "        f.write(\"host = localhost\\n\")\n",
        "        f.write(\"port = 5432\\n\")\n",
        "        f.write(\"[user]\\n\")\n",
        "        f.write(\"theme = dark\\n\")\n",
        "\n",
        "    # XML\n",
        "    xml_path = os.path.join(base_dir, \"data.xml\")\n",
        "    with open(xml_path, \"w\") as f:\n",
        "        f.write(\"<data><item><id>1</id><name>Laptop</name></item><item><id>2</id><name>Mouse</name></item></data>\")\n",
        "\n",
        "    # Create a zip file containing another data file\n",
        "    json_for_zip_path = os.path.join(base_dir, \"product.json\")\n",
        "    with open(json_for_zip_path, 'w') as f:\n",
        "        json.dump({\"product_id\": \"xyz-123\", \"stock\": 42}, f)\n",
        "\n",
        "    zip_path = os.path.join(base_dir, \"archive.zip\")\n",
        "    with zipfile.ZipFile(zip_path, 'w') as zf:\n",
        "        zf.write(json_for_zip_path, os.path.basename(json_for_zip_path))\n",
        "\n",
        "    # 3. Process the files\n",
        "    files_to_process = [csv_path, ini_path, xml_path, zip_path]\n",
        "    all_parsed_data = {}\n",
        "\n",
        "    for path in files_to_process:\n",
        "        # Check if it's an archive and extract it\n",
        "        extraction_path = unzip_file_if_needed(path, extracted_dir)\n",
        "\n",
        "        if extraction_path:\n",
        "            # If it was an archive, parse the files inside it\n",
        "            for root, _, files in os.walk(extraction_path):\n",
        "                for name in files:\n",
        "                    file_to_parse = os.path.join(root, name)\n",
        "                    parsed_data = parse_file_to_json(file_to_parse)\n",
        "                    if parsed_data:\n",
        "                        all_parsed_data[name] = parsed_data\n",
        "        else:\n",
        "            # Otherwise, parse the file directly\n",
        "            parsed_data = parse_file_to_json(path)\n",
        "            if parsed_data:\n",
        "                all_parsed_data[os.path.basename(path)] = parsed_data\n",
        "\n",
        "    # 4. Print the final combined JSON output\n",
        "    print(\"\\n\\n--- COMBINED PARSED DATA (JSON) ---\")\n",
        "    # Use indent for pretty-printing\n",
        "    final_json = json.dumps(all_parsed_data, indent=4)\n",
        "    print(final_json)\n",
        "\n",
        "    # 5. Save the final JSON to a file\n",
        "    output_json_path = os.path.join(base_dir, \"output.json\")\n",
        "    with open(output_json_path, 'w') as f:\n",
        "        f.write(final_json)\n",
        "    print(f\"\\n--- Saved combined JSON to '{output_json_path}' ---\")"
      ],
      "metadata": {
        "id": "XTcB_c31_QBI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba0fc38e-c3e8-406b-9a4b-b6206ecb7f25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing 'users.csv'...\n",
            "Parsing 'settings.ini'...\n",
            "Parsing 'data.xml'...\n",
            "Archive detected: 'archive.zip'. Extracting...\n",
            "Successfully extracted to 'parsing_test/extracted/archive'\n",
            "Parsing 'product.json'...\n",
            "\n",
            "\n",
            "--- COMBINED PARSED DATA (JSON) ---\n",
            "{\n",
            "    \"users.csv\": [\n",
            "        {\n",
            "            \"id\": \"1\",\n",
            "            \"name\": \"Alice\",\n",
            "            \"email\": \"alice@example.com\"\n",
            "        },\n",
            "        {\n",
            "            \"id\": \"2\",\n",
            "            \"name\": \"Bob\",\n",
            "            \"email\": \"bob@example.com\"\n",
            "        }\n",
            "    ],\n",
            "    \"settings.ini\": {\n",
            "        \"database\": {\n",
            "            \"host\": \"localhost\",\n",
            "            \"port\": \"5432\"\n",
            "        },\n",
            "        \"user\": {\n",
            "            \"theme\": \"dark\"\n",
            "        }\n",
            "    },\n",
            "    \"data.xml\": {\n",
            "        \"data\": {\n",
            "            \"item\": [\n",
            "                {\n",
            "                    \"id\": \"1\",\n",
            "                    \"name\": \"Laptop\"\n",
            "                },\n",
            "                {\n",
            "                    \"id\": \"2\",\n",
            "                    \"name\": \"Mouse\"\n",
            "                }\n",
            "            ]\n",
            "        }\n",
            "    },\n",
            "    \"product.json\": {\n",
            "        \"product_id\": \"xyz-123\",\n",
            "        \"stock\": 42\n",
            "    }\n",
            "}\n",
            "\n",
            "--- Saved combined JSON to 'parsing_test/output.json' ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SCHEMA CHECK:** CHECK TO SEE IF THE SCHEMA MATCHES."
      ],
      "metadata": {
        "id": "MghJvEvK_eIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# 1. DEFINE YOUR INTERNAL, CANONICAL SCHEMA\n",
        "# This is the target schema that all data must conform to after transformation.\n",
        "TARGET_SCHEMA = {\n",
        "    \"external_job_id\": {\"type\": str, \"required\": True},\n",
        "    \"job_source\": {\"type\": str, \"required\": True, \"allowed_values\": [\"COMPANY_WEBSITE\", \"JOB_FEED\"]},\n",
        "    \"feed_id\": {\"type\": int, \"required\": False, \"nullable\": True},\n",
        "    \"created_at\": {\"type\": \"datetime\", \"required\": True},\n",
        "    \"updated_at\": {\"type\": \"datetime\", \"required\": True},\n",
        "    \"posted_at\": {\"type\": \"datetime\", \"required\": True},\n",
        "    \"expires_at\": {\"type\": \"datetime\", \"required\": False, \"nullable\": True},\n",
        "    \"status\": {\"type\": str, \"required\": True},\n",
        "    \"company_name\": {\"type\": str, \"required\": True},\n",
        "    \"title\": {\"type\": str, \"required\": True},\n",
        "    \"description\": {\"type\": str, \"required\": True},\n",
        "    \"application_url\": {\"type\": str, \"required\": False, \"nullable\": True},\n",
        "    \"employment_type\": {\"type\": str, \"required\": False, \"nullable\": True},\n",
        "    \"is_remote\": {\"type\": bool, \"required\": True},\n",
        "    \"is_multi_location\": {\"type\": bool, \"required\": True},\n",
        "    \"is_international\": {\"type\": bool, \"required\": True},\n",
        "    \"locations\": {\"type\": list, \"required\": False, \"nullable\": True},\n",
        "    \"salary_min\": {\"type\": (int, float), \"required\": False, \"nullable\": True},\n",
        "    \"salary_max\": {\"type\": (int, float), \"required\": False, \"nullable\": True},\n",
        "    \"salary_period\": {\"type\": str, \"required\": False, \"nullable\": True},\n",
        "    \"currency\": {\"type\": str, \"required\": False, \"nullable\": True},\n",
        "}\n",
        "\n",
        "# 2. DEFINE THE EXPANDED, COMPREHENSIVE MAPPING FROM VARIOUS FEED SCHEMAS\n",
        "# Maps a huge variety of possible feed field names (key) to your internal field name (value).\n",
        "FEED_SCHEMA_MAPPING = {\n",
        "    # --- Company Name Mappings ---\n",
        "    'company': 'company_name',\n",
        "    'company_name': 'company_name',\n",
        "    'companyName': 'company_name',\n",
        "    'hiring_organization': 'company_name',\n",
        "    'hiringOrganization': 'company_name',\n",
        "    'employer': 'company_name',\n",
        "\n",
        "    # --- Description Mappings ---\n",
        "    'body': 'description',\n",
        "    'description': 'description',\n",
        "    'jobDescription': 'description',\n",
        "    'job_description': 'description',\n",
        "    'full_description': 'description',\n",
        "    'details': 'description',\n",
        "    'job_details': 'description',\n",
        "\n",
        "    # --- Posting Date Mappings ---\n",
        "    'date': 'posted_at',\n",
        "    'posted_at': 'posted_at',\n",
        "    'datePosted': 'posted_at',\n",
        "    'date_posted': 'posted_at',\n",
        "    'publication_date': 'posted_at',\n",
        "    'post_date': 'posted_at',\n",
        "\n",
        "    # --- Application URL Mappings ---\n",
        "    'url': 'application_url',\n",
        "    'job_url': 'application_url',\n",
        "    'applyLink': 'application_url',\n",
        "    'application_link': 'application_url',\n",
        "    'apply_url': 'application_url',\n",
        "    'link': 'application_url',\n",
        "\n",
        "    # --- Job Title Mappings ---\n",
        "    'title': 'title',\n",
        "    'jobTitle': 'title',\n",
        "    'job_title': 'title',\n",
        "    'position_title': 'title',\n",
        "    'position': 'title',\n",
        "    'role': 'title',\n",
        "\n",
        "    # --- Location Mappings ---\n",
        "    'location': 'locations',\n",
        "    'jobLocations': 'locations',\n",
        "    'job_location': 'locations',\n",
        "    'address': 'locations',\n",
        "    'work_location': 'locations',\n",
        "    'city_state': 'locations',\n",
        "    'city': 'locations',\n",
        "    'state': 'locations',\n",
        "    'country': 'locations',\n",
        "\n",
        "    # --- Employment Type Mappings ---\n",
        "    'job-type': 'employment_type',\n",
        "    'job_type': 'employment_type',\n",
        "    'jobType': 'employment_type',\n",
        "    'type': 'employment_type',\n",
        "    'position_type': 'employment_type',\n",
        "    'contract_type': 'employment_type',\n",
        "    'employmentType': 'employment_type',\n",
        "\n",
        "    # --- External ID Mappings ---\n",
        "    'referencenumber': 'external_job_id',\n",
        "    'ref_id': 'external_job_id',\n",
        "    'jobID': 'external_job_id',\n",
        "    'job_id': 'external_job_id',\n",
        "    'reference_id': 'external_job_id',\n",
        "    'requisition_id': 'external_job_id',\n",
        "    'job_reference': 'external_job_id',\n",
        "\n",
        "    # --- Remote Flag Mapping ---\n",
        "    'remote': 'is_remote',\n",
        "    'is_remote': 'is_remote',\n",
        "    'isRemote': 'is_remote',\n",
        "\n",
        "    # --- Salary Mappings ---\n",
        "    'salary_min': 'salary_min',\n",
        "    'min_salary': 'salary_min',\n",
        "    'minimum_salary': 'salary_min',\n",
        "    'salary_from': 'salary_min',\n",
        "    'salary_max': 'salary_max',\n",
        "    'max_salary': 'salary_max',\n",
        "    'maximum_salary': 'salary_max',\n",
        "    'salary_to': 'salary_max',\n",
        "    'salary_period': 'salary_period',\n",
        "    'salary_frequency': 'salary_period',\n",
        "    'pay_period': 'salary_period',\n",
        "    'currency': 'currency',\n",
        "    'salary_currency': 'currency',\n",
        "}\n",
        "\n",
        "def transform_job_data(raw_data, mapping):\n",
        "    \"\"\"\n",
        "    Transforms raw data from a feed into our internal schema format using a mapping.\n",
        "    \"\"\"\n",
        "    transformed_data = {}\n",
        "    for raw_key, raw_value in raw_data.items():\n",
        "        # If the key is in our mapping, use the mapped key.\n",
        "        # Otherwise, use the original key (for fields that already match).\n",
        "        target_key = mapping.get(raw_key, raw_key)\n",
        "\n",
        "        # Only include fields that are part of our target schema\n",
        "        if target_key in TARGET_SCHEMA:\n",
        "            transformed_data[target_key] = raw_value\n",
        "\n",
        "    return transformed_data\n",
        "\n",
        "def validate_datetime_string(dt_string):\n",
        "    \"\"\"Checks if a string is a valid ISO 8601 format.\"\"\"\n",
        "    try:\n",
        "        datetime.fromisoformat(dt_string.replace('Z', '+00:00'))\n",
        "        return True\n",
        "    except (ValueError, TypeError):\n",
        "        return False\n",
        "\n",
        "def check_schema(job_data):\n",
        "    \"\"\"\n",
        "    Validates a transformed job data dictionary against the TARGET_SCHEMA.\n",
        "    \"\"\"\n",
        "    errors = []\n",
        "    for field, rules in TARGET_SCHEMA.items():\n",
        "        if rules.get(\"required\") and field not in job_data:\n",
        "            errors.append(f\"Missing required field: '{field}'\")\n",
        "    if errors: return False, errors\n",
        "\n",
        "    for field, value in job_data.items():\n",
        "        if field in TARGET_SCHEMA:\n",
        "            rules = TARGET_SCHEMA[field]\n",
        "            expected_type = rules[\"type\"]\n",
        "            if value is None:\n",
        "                if not rules.get(\"nullable\"):\n",
        "                    errors.append(f\"Field '{field}' cannot be null.\")\n",
        "                continue\n",
        "            if expected_type == \"datetime\":\n",
        "                if not isinstance(value, str) or not validate_datetime_string(value):\n",
        "                    errors.append(f\"Field '{field}' is not a valid ISO datetime string. Got: {value}\")\n",
        "                continue\n",
        "            if not isinstance(value, expected_type):\n",
        "                errors.append(f\"Field '{field}' has incorrect type. Expected {expected_type}, got {type(value)}.\")\n",
        "            if \"allowed_values\" in rules and value not in rules[\"allowed_values\"]:\n",
        "                errors.append(f\"Field '{field}' has value '{value}', but only {rules['allowed_values']} are allowed.\")\n",
        "\n",
        "    if job_data.get(\"job_source\") == \"JOB_FEED\" and job_data.get(\"feed_id\") is None:\n",
        "        errors.append(\"Conditional error: 'feed_id' is required when 'job_source' is 'JOB_FEED'.\")\n",
        "    if job_data.get(\"job_source\") == \"COMPANY_WEBSITE\" and job_data.get(\"feed_id\") is not None:\n",
        "        errors.append(\"Conditional error: 'feed_id' must be null when 'job_source' is 'COMPANY_WEBSITE'.\")\n",
        "\n",
        "    return not errors, errors\n",
        "\n",
        "\n",
        "# --- Main execution block to demonstrate the full process ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Example of a raw job data object from a feed using varied field names\n",
        "    raw_job_from_feed = {\n",
        "        \"requisition_id\": \"feed-xyz-123\", # Mapped to external_job_id\n",
        "        \"position_title\": \"Lead DevOps Engineer\", # Mapped to title\n",
        "        \"job_details\": \"Manage our cloud infrastructure on AWS.\", # Mapped to description\n",
        "        \"hiring_organization\": \"CloudScale Inc.\", # Mapped to company_name\n",
        "        \"publication_date\": \"2023-10-28T12:00:00Z\", # Mapped to posted_at\n",
        "        \"apply_url\": \"https://cloudscale.jobs/apply/xyz-123\", # Mapped to application_url\n",
        "        \"employmentType\": \"FULL_TIME\", # Mapped to employment_type\n",
        "        \"isRemote\": True, # Mapped to is_remote\n",
        "        \"address\": [{\"city\": \"Remote\"}], # Mapped to 'locations'\n",
        "        \"feed_source_name\": \"Generic Job Feed\", # This extra field will be ignored\n",
        "        \"cpc\": \"0.50\" # This extra field will also be ignored\n",
        "    }\n",
        "\n",
        "    print(\"--- 1. RAW JOB DATA FROM FEED ---\")\n",
        "    print(json.dumps(raw_job_from_feed, indent=2))\n",
        "\n",
        "    # 1. Transform the raw data using the mapping\n",
        "    transformed_job = transform_job_data(raw_job_from_feed, FEED_SCHEMA_MAPPING)\n",
        "\n",
        "    print(\"\\n--- 2. TRANSFORMED JOB DATA (ready for validation) ---\")\n",
        "    print(json.dumps(transformed_job, indent=2))\n",
        "\n",
        "    # 2. Add required fields that are not in the feed (e.g., metadata)\n",
        "    # This data would come from the context of the running job.\n",
        "    transformed_job['job_source'] = 'JOB_FEED'\n",
        "    transformed_job['feed_id'] = 101\n",
        "    transformed_job['status'] = 'ACTIVE'\n",
        "    transformed_job['created_at'] = datetime.utcnow().isoformat() + 'Z'\n",
        "    transformed_job['updated_at'] = datetime.utcnow().isoformat() + 'Z'\n",
        "    # These boolean fields might need to be inferred or set to defaults\n",
        "    transformed_job['is_multi_location'] = False\n",
        "    transformed_job['is_international'] = False\n",
        "\n",
        "    print(\"\\n--- 3. FINAL JOB OBJECT (after adding metadata) ---\")\n",
        "    print(json.dumps(transformed_job, indent=2))\n",
        "\n",
        "    # 3. Validate the final, transformed data object\n",
        "    is_valid, validation_errors = check_schema(transformed_job)\n",
        "\n",
        "    print(\"\\n--- 4. VALIDATION RESULT ---\")\n",
        "    if is_valid:\n",
        "        print(\"Result: VALID\")\n",
        "        print(\"This job object is clean, validated, and ready to be processed further.\")\n",
        "    else:\n",
        "        print(f\"Result: INVALID. Errors:\\n{json.dumps(validation_errors, indent=2)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "cy0_fJ13_xd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb80fd34-7d14-4922-cc65-5843d3fcd6a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. RAW JOB DATA FROM FEED ---\n",
            "{\n",
            "  \"requisition_id\": \"feed-xyz-123\",\n",
            "  \"position_title\": \"Lead DevOps Engineer\",\n",
            "  \"job_details\": \"Manage our cloud infrastructure on AWS.\",\n",
            "  \"hiring_organization\": \"CloudScale Inc.\",\n",
            "  \"publication_date\": \"2023-10-28T12:00:00Z\",\n",
            "  \"apply_url\": \"https://cloudscale.jobs/apply/xyz-123\",\n",
            "  \"employmentType\": \"FULL_TIME\",\n",
            "  \"isRemote\": true,\n",
            "  \"address\": [\n",
            "    {\n",
            "      \"city\": \"Remote\"\n",
            "    }\n",
            "  ],\n",
            "  \"feed_source_name\": \"Generic Job Feed\",\n",
            "  \"cpc\": \"0.50\"\n",
            "}\n",
            "\n",
            "--- 2. TRANSFORMED JOB DATA (ready for validation) ---\n",
            "{\n",
            "  \"external_job_id\": \"feed-xyz-123\",\n",
            "  \"title\": \"Lead DevOps Engineer\",\n",
            "  \"description\": \"Manage our cloud infrastructure on AWS.\",\n",
            "  \"company_name\": \"CloudScale Inc.\",\n",
            "  \"posted_at\": \"2023-10-28T12:00:00Z\",\n",
            "  \"application_url\": \"https://cloudscale.jobs/apply/xyz-123\",\n",
            "  \"employment_type\": \"FULL_TIME\",\n",
            "  \"is_remote\": true,\n",
            "  \"locations\": [\n",
            "    {\n",
            "      \"city\": \"Remote\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "--- 3. FINAL JOB OBJECT (after adding metadata) ---\n",
            "{\n",
            "  \"external_job_id\": \"feed-xyz-123\",\n",
            "  \"title\": \"Lead DevOps Engineer\",\n",
            "  \"description\": \"Manage our cloud infrastructure on AWS.\",\n",
            "  \"company_name\": \"CloudScale Inc.\",\n",
            "  \"posted_at\": \"2023-10-28T12:00:00Z\",\n",
            "  \"application_url\": \"https://cloudscale.jobs/apply/xyz-123\",\n",
            "  \"employment_type\": \"FULL_TIME\",\n",
            "  \"is_remote\": true,\n",
            "  \"locations\": [\n",
            "    {\n",
            "      \"city\": \"Remote\"\n",
            "    }\n",
            "  ],\n",
            "  \"job_source\": \"JOB_FEED\",\n",
            "  \"feed_id\": 101,\n",
            "  \"status\": \"ACTIVE\",\n",
            "  \"created_at\": \"2025-07-24T10:56:45.596561Z\",\n",
            "  \"updated_at\": \"2025-07-24T10:56:45.596578Z\",\n",
            "  \"is_multi_location\": false,\n",
            "  \"is_international\": false\n",
            "}\n",
            "\n",
            "--- 4. VALIDATION RESULT ---\n",
            "Result: VALID\n",
            "This job object is clean, validated, and ready to be processed further.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXTRACTION:** MOVE JOB INTO SUPABASE"
      ],
      "metadata": {
        "id": "NuhS1bSg_eOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Assume you have supabase-py installed: pip install supabase\n",
        "# from supabase import create_client, Client\n",
        "\n",
        "# --- Mock Supabase Client for Demonstration ---\n",
        "# In a real application, you would initialize a real Supabase client like this:\n",
        "# SUPABASE_URL = os.environ.get(\"SUPABASE_URL\")\n",
        "# SUPABASE_KEY = os.environ.get(\"SUPABASE_KEY\")\n",
        "# supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "class MockSupabaseClient:\n",
        "    \"\"\"A mock client to simulate Supabase interactions for demonstration.\"\"\"\n",
        "    def table(self, table_name):\n",
        "        print(f\"[Supabase] Accessing table: '{table_name}'\")\n",
        "        return self\n",
        "\n",
        "    def insert(self, data):\n",
        "        print(f\"[Supabase] Inserting data...\")\n",
        "        # Pretty-print the data that would be sent to Supabase\n",
        "        print(json.dumps(data, indent=2))\n",
        "        # Simulate a successful response\n",
        "        class MockResponse:\n",
        "            data = [{\"status\": \"inserted\"}]\n",
        "        return MockResponse()\n",
        "\n",
        "# Initialize the mock client\n",
        "supabase = MockSupabaseClient()\n",
        "\n",
        "# --- Schema Definition and Validation (from SCHEMA_CHECK.py) ---\n",
        "\n",
        "TARGET_SCHEMA = {\n",
        "    \"external_job_id\": {\"type\": str, \"required\": True},\n",
        "    \"job_source\": {\"type\": str, \"required\": True, \"allowed_values\": [\"COMPANY_WEBSITE\", \"JOB_FEED\"]},\n",
        "    \"feed_id\": {\"type\": int, \"required\": False, \"nullable\": True},\n",
        "    \"created_at\": {\"type\": \"datetime\", \"required\": True},\n",
        "    \"updated_at\": {\"type\": \"datetime\", \"required\": True},\n",
        "    \"posted_at\": {\"type\": \"datetime\", \"required\": True},\n",
        "    \"expires_at\": {\"type\": \"datetime\", \"required\": False, \"nullable\": True},\n",
        "    \"status\": {\"type\": str, \"required\": True},\n",
        "    \"company_name\": {\"type\": str, \"required\": True},\n",
        "    \"title\": {\"type\": str, \"required\": True},\n",
        "    \"description\": {\"type\": str, \"required\": True},\n",
        "    \"application_url\": {\"type\": str, \"required\": False, \"nullable\": True},\n",
        "    \"employment_type\": {\"type\": str, \"required\": False, \"nullable\": True},\n",
        "    \"is_remote\": {\"type\": bool, \"required\": True},\n",
        "    \"is_multi_location\": {\"type\": bool, \"required\": True},\n",
        "    \"is_international\": {\"type\": bool, \"required\": True},\n",
        "    \"locations\": {\"type\": list, \"required\": False, \"nullable\": True},\n",
        "    \"salary_min\": {\"type\": (int, float), \"required\": False, \"nullable\": True},\n",
        "    \"salary_max\": {\"type\": (int, float), \"required\": False, \"nullable\": True},\n",
        "    \"salary_period\": {\"type\": str, \"required\": False, \"nullable\": True},\n",
        "    \"currency\": {\"type\": str, \"required\": False, \"nullable\": True},\n",
        "}\n",
        "\n",
        "def validate_datetime_string(dt_string):\n",
        "    \"\"\"Checks if a string is a valid ISO 8601 format.\"\"\"\n",
        "    try:\n",
        "        datetime.fromisoformat(dt_string.replace('Z', '+00:00'))\n",
        "        return True\n",
        "    except (ValueError, TypeError):\n",
        "        return False\n",
        "\n",
        "def check_schema(job_data):\n",
        "    \"\"\"Validates a job data dictionary against the TARGET_SCHEMA.\"\"\"\n",
        "    errors = []\n",
        "    for field, rules in TARGET_SCHEMA.items():\n",
        "        if rules.get(\"required\") and field not in job_data:\n",
        "            errors.append(f\"Missing required field: '{field}'\")\n",
        "    if errors:\n",
        "        return False, errors\n",
        "\n",
        "    for field, value in job_data.items():\n",
        "        if field in TARGET_SCHEMA:\n",
        "            rules = TARGET_SCHEMA[field]\n",
        "            expected_type = rules[\"type\"]\n",
        "            if value is None:\n",
        "                if not rules.get(\"nullable\"):\n",
        "                    errors.append(f\"Field '{field}' cannot be null.\")\n",
        "                continue\n",
        "            if expected_type == \"datetime\":\n",
        "                if not isinstance(value, str) or not validate_datetime_string(value):\n",
        "                    errors.append(f\"Field '{field}' is not a valid ISO datetime string. Got: {value}\")\n",
        "                continue\n",
        "            if not isinstance(value, expected_type):\n",
        "                errors.append(f\"Field '{field}' has incorrect type. Expected {expected_type}, got {type(value)}.\")\n",
        "            if \"allowed_values\" in rules and value not in rules[\"allowed_values\"]:\n",
        "                errors.append(f\"Field '{field}' has value '{value}', but only {rules['allowed_values']} are allowed.\")\n",
        "\n",
        "    if job_data.get(\"job_source\") == \"JOB_FEED\" and job_data.get(\"feed_id\") is None:\n",
        "        errors.append(\"Conditional error: 'feed_id' is required when 'job_source' is 'JOB_FEED'.\")\n",
        "    if job_data.get(\"job_source\") == \"COMPANY_WEBSITE\" and job_data.get(\"feed_id\") is not None:\n",
        "        errors.append(\"Conditional error: 'feed_id' must be null when 'job_source' is 'COMPANY_WEBSITE'.\")\n",
        "\n",
        "    return not errors, errors\n",
        "\n",
        "# --- Data Extraction and Insertion Logic ---\n",
        "\n",
        "def extract_and_load_job(job_data, table_name=\"open_jobs\"):\n",
        "    \"\"\"\n",
        "    Performs schema check, extracts valid fields, and loads them into a Supabase table.\n",
        "\n",
        "    Args:\n",
        "        job_data (dict): The raw job data from a feed or other source.\n",
        "        table_name (str): The name of the Supabase table to insert into.\n",
        "    \"\"\"\n",
        "    print(f\"--- Processing job with external_id: {job_data.get('external_job_id', 'N/A')} ---\")\n",
        "\n",
        "    # 1. Validate the data against the schema\n",
        "    is_valid, errors = check_schema(job_data)\n",
        "\n",
        "    if not is_valid:\n",
        "        print(f\"Validation FAILED. Errors: {errors}\")\n",
        "        print(\"Job will not be loaded.\\n\")\n",
        "        return\n",
        "\n",
        "    print(\"Validation PASSED.\")\n",
        "\n",
        "    # 2. Extract only the fields defined in our schema\n",
        "    extracted_data = {key: job_data[key] for key in TARGET_SCHEMA if key in job_data}\n",
        "\n",
        "    print(\"Extracted data matching schema:\")\n",
        "    print(json.dumps(extracted_data, indent=2))\n",
        "\n",
        "    # 3. Load the clean data into the Supabase table\n",
        "    try:\n",
        "        response = supabase.table(table_name).insert(extracted_data)\n",
        "        if response.data:\n",
        "            print(f\"Successfully loaded job into '{table_name}'.\\n\")\n",
        "        else:\n",
        "            print(\"Failed to load job into Supabase.\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading data to Supabase: {e}\\n\")\n",
        "\n",
        "\n",
        "# --- Main execution block to demonstrate the function ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Example of a valid job from a feed with extra fields\n",
        "    valid_job_from_feed = {\n",
        "        \"external_job_id\": \"gh_1a2b3c4d\",\n",
        "        \"job_source\": \"JOB_FEED\",\n",
        "        \"feed_id\": 3,\n",
        "        \"created_at\": \"2023-10-27T10:00:00Z\",\n",
        "        \"updated_at\": \"2023-10-27T11:30:00Z\",\n",
        "        \"posted_at\": \"2023-10-26T09:00:00Z\",\n",
        "        \"expires_at\": \"2023-11-25T23:59:59Z\",\n",
        "        \"status\": \"ACTIVE\",\n",
        "        \"company_name\": \"Innovatech Solutions Inc.\",\n",
        "        \"title\": \"Senior Software Engineer (Backend), Platform Team\",\n",
        "        \"description\": \"<div><strong>About us:</strong>...</div>\",\n",
        "        \"application_url\": \"https://jobs.innovatech.com/apply/1a2b3c4d\",\n",
        "        \"employment_type\": \"FULL_TIME\",\n",
        "        \"is_remote\": False,\n",
        "        \"is_multi_location\": True,\n",
        "        \"is_international\": False,\n",
        "        \"locations\": [{\"location_id\": 101, \"city\": \"San Francisco\", \"state\": \"CA\", \"country\": \"USA\"}],\n",
        "        \"salary_min\": 150000,\n",
        "        \"salary_max\": 180000,\n",
        "        \"salary_period\": \"YEARLY\",\n",
        "        \"currency\": \"USD\",\n",
        "        \"feed_specific_field\": \"value123\", # This will be ignored\n",
        "        \"source_priority\": \"High\" # This will also be ignored\n",
        "    }\n",
        "\n",
        "    # Example of an invalid job that will be rejected\n",
        "    invalid_job = {\n",
        "        \"external_job_id\": \"gh_5e6f7g8h\",\n",
        "        \"job_source\": \"JOB_FEED\",\n",
        "        # feed_id is missing, which will cause a validation error\n",
        "        \"created_at\": \"2023-10-27T10:00:00Z\",\n",
        "        \"updated_at\": \"2023-10-27T11:30:00Z\",\n",
        "        \"posted_at\": \"2023-10-26T09:00:00Z\",\n",
        "        \"status\": \"ACTIVE\",\n",
        "        \"company_name\": \"Data Corp\",\n",
        "        \"title\": \"Data Analyst\",\n",
        "        \"description\": \"<p>Looking for a data analyst.</p>\",\n",
        "        \"is_remote\": True,\n",
        "        \"is_multi_location\": False,\n",
        "        \"is_international\": False,\n",
        "        \"locations\": []\n",
        "    }\n",
        "\n",
        "    # Process both jobs\n",
        "    extract_and_load_job(valid_job_from_feed)\n",
        "    extract_and_load_job(invalid_job)\n"
      ],
      "metadata": {
        "id": "qGiHj2AmC8ZQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "782ae471-b074-4aa5-a5f9-4047b9f47d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Processing job with external_id: gh_1a2b3c4d ---\n",
            "Validation PASSED.\n",
            "Extracted data matching schema:\n",
            "{\n",
            "  \"external_job_id\": \"gh_1a2b3c4d\",\n",
            "  \"job_source\": \"JOB_FEED\",\n",
            "  \"feed_id\": 3,\n",
            "  \"created_at\": \"2023-10-27T10:00:00Z\",\n",
            "  \"updated_at\": \"2023-10-27T11:30:00Z\",\n",
            "  \"posted_at\": \"2023-10-26T09:00:00Z\",\n",
            "  \"expires_at\": \"2023-11-25T23:59:59Z\",\n",
            "  \"status\": \"ACTIVE\",\n",
            "  \"company_name\": \"Innovatech Solutions Inc.\",\n",
            "  \"title\": \"Senior Software Engineer (Backend), Platform Team\",\n",
            "  \"description\": \"<div><strong>About us:</strong>...</div>\",\n",
            "  \"application_url\": \"https://jobs.innovatech.com/apply/1a2b3c4d\",\n",
            "  \"employment_type\": \"FULL_TIME\",\n",
            "  \"is_remote\": false,\n",
            "  \"is_multi_location\": true,\n",
            "  \"is_international\": false,\n",
            "  \"locations\": [\n",
            "    {\n",
            "      \"location_id\": 101,\n",
            "      \"city\": \"San Francisco\",\n",
            "      \"state\": \"CA\",\n",
            "      \"country\": \"USA\"\n",
            "    }\n",
            "  ],\n",
            "  \"salary_min\": 150000,\n",
            "  \"salary_max\": 180000,\n",
            "  \"salary_period\": \"YEARLY\",\n",
            "  \"currency\": \"USD\"\n",
            "}\n",
            "[Supabase] Accessing table: 'open_jobs'\n",
            "[Supabase] Inserting data...\n",
            "{\n",
            "  \"external_job_id\": \"gh_1a2b3c4d\",\n",
            "  \"job_source\": \"JOB_FEED\",\n",
            "  \"feed_id\": 3,\n",
            "  \"created_at\": \"2023-10-27T10:00:00Z\",\n",
            "  \"updated_at\": \"2023-10-27T11:30:00Z\",\n",
            "  \"posted_at\": \"2023-10-26T09:00:00Z\",\n",
            "  \"expires_at\": \"2023-11-25T23:59:59Z\",\n",
            "  \"status\": \"ACTIVE\",\n",
            "  \"company_name\": \"Innovatech Solutions Inc.\",\n",
            "  \"title\": \"Senior Software Engineer (Backend), Platform Team\",\n",
            "  \"description\": \"<div><strong>About us:</strong>...</div>\",\n",
            "  \"application_url\": \"https://jobs.innovatech.com/apply/1a2b3c4d\",\n",
            "  \"employment_type\": \"FULL_TIME\",\n",
            "  \"is_remote\": false,\n",
            "  \"is_multi_location\": true,\n",
            "  \"is_international\": false,\n",
            "  \"locations\": [\n",
            "    {\n",
            "      \"location_id\": 101,\n",
            "      \"city\": \"San Francisco\",\n",
            "      \"state\": \"CA\",\n",
            "      \"country\": \"USA\"\n",
            "    }\n",
            "  ],\n",
            "  \"salary_min\": 150000,\n",
            "  \"salary_max\": 180000,\n",
            "  \"salary_period\": \"YEARLY\",\n",
            "  \"currency\": \"USD\"\n",
            "}\n",
            "Successfully loaded job into 'open_jobs'.\n",
            "\n",
            "--- Processing job with external_id: gh_5e6f7g8h ---\n",
            "Validation FAILED. Errors: [\"Conditional error: 'feed_id' is required when 'job_source' is 'JOB_FEED'.\"]\n",
            "Job will not be loaded.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BASE64:** CONVERT JOB TO BASE64."
      ],
      "metadata": {
        "id": "zZstI41v_eQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import base64\n",
        "import hashlib\n",
        "\n",
        "def get_canonical_job_hash(job_data):\n",
        "    \"\"\"\n",
        "    Creates a unique and stable Base64 hash for a job object by focusing\n",
        "    on its core, location-independent content.\n",
        "\n",
        "    This function ignores variations in external IDs, locations, and application URLs\n",
        "    to identify true duplicates.\n",
        "\n",
        "    Args:\n",
        "        job_data (dict): The job data dictionary.\n",
        "\n",
        "    Returns:\n",
        "        str: A unique Base64 encoded SHA256 hash representing the job's core content.\n",
        "    \"\"\"\n",
        "    # 1. Define the core fields that truly determine the uniqueness of a job,\n",
        "    # independent of its location or source-specific IDs.\n",
        "    uniqueness_fields = [\n",
        "        'company_name',\n",
        "        'title',\n",
        "        'description',\n",
        "        'employment_type',\n",
        "    ]\n",
        "\n",
        "    # Create a dictionary with only the core fields.\n",
        "    # Use .get() to avoid errors if a field is missing, defaulting to None.\n",
        "    canonical_dict = {key: job_data.get(key) for key in uniqueness_fields}\n",
        "\n",
        "    # NOTE: We explicitly DO NOT include 'external_job_id', 'locations', 'is_remote',\n",
        "    # or 'application_url' to ensure jobs are deduplicated correctly even with\n",
        "    # variations in those fields.\n",
        "\n",
        "    # 2. Create a stable, sorted JSON string.\n",
        "    # Sorting the keys ensures that {\"a\": 1, \"b\": 2} and {\"b\": 2, \"a\": 1} produce the same hash.\n",
        "    # Using separators without whitespace makes the output compact and consistent.\n",
        "    canonical_string = json.dumps(canonical_dict, sort_keys=True, separators=(',', ':'))\n",
        "\n",
        "    # 3. Hash the string using SHA256 for a secure, fixed-length output.\n",
        "    hash_object = hashlib.sha256(canonical_string.encode('utf-8'))\n",
        "\n",
        "    # 4. Encode the binary hash in Base64 for easy storage and comparison.\n",
        "    base64_hash = base64.b64encode(hash_object.digest()).decode('utf-8')\n",
        "\n",
        "    return base64_hash\n",
        "\n",
        "# --- Main execution block to demonstrate the function ---\n",
        "if __name__ == \"__main__\":\n",
        "    # This is the base job.\n",
        "    job_a = {\n",
        "        \"external_job_id\": \"gh_1a2b3c4d\", # Will be ignored\n",
        "        \"company_name\": \"Innovatech Solutions Inc.\",\n",
        "        \"title\": \"Senior Software Engineer (Backend)\",\n",
        "        \"description\": \"Join our platform team to build scalable microservices.\",\n",
        "        \"employment_type\": \"FULL_TIME\",\n",
        "        \"locations\": [{\"city\": \"San Francisco\", \"state\": \"CA\"}], # Will be ignored\n",
        "        \"application_url\": \"https://example.com/apply?source=1\" # Will be ignored\n",
        "    }\n",
        "\n",
        "    # This job is identical in its core content but has a different ID and location.\n",
        "    # It should produce the SAME hash.\n",
        "    job_b_same_core_content = {\n",
        "        \"external_job_id\": \"indeed_xyz987\", # Different ID\n",
        "        \"company_name\": \"Innovatech Solutions Inc.\",\n",
        "        \"title\": \"Senior Software Engineer (Backend)\",\n",
        "        \"description\": \"Join our platform team to build scalable microservices.\",\n",
        "        \"employment_type\": \"FULL_TIME\",\n",
        "        \"locations\": [{\"city\": \"New York\", \"state\": \"NY\"}, {\"city\": \"Remote\"}], # Different locations\n",
        "        \"application_url\": \"https://example.com/apply?source=2\" # Different URL\n",
        "    }\n",
        "\n",
        "    # This job is genuinely different.\n",
        "    job_c_different_job = {\n",
        "        \"external_job_id\": \"gh_5e6f7g8h\",\n",
        "        \"company_name\": \"Data Corp\",\n",
        "        \"title\": \"Data Analyst\",\n",
        "        \"description\": \"<p>Looking for a data analyst.</p>\",\n",
        "        \"employment_type\": \"FULL_TIME\",\n",
        "        \"locations\": []\n",
        "    }\n",
        "\n",
        "    hash_a = get_canonical_job_hash(job_a)\n",
        "    hash_b = get_canonical_job_hash(job_b_same_core_content)\n",
        "    hash_c = get_canonical_job_hash(job_c_different_job)\n",
        "\n",
        "    print(f\"Job A Hash: {hash_a}\")\n",
        "    print(f\"Job B Hash: {hash_b}\")\n",
        "    print(f\"Job C Hash: {hash_c}\")\n",
        "\n",
        "    print(\"\\n--- Verification ---\")\n",
        "    print(f\"Hash A and B are identical (as expected): {hash_a == hash_b}\")\n",
        "    print(f\"Hash A and C are different (as expected): {hash_a != hash_c}\")\n"
      ],
      "metadata": {
        "id": "vwEyX-6HGD0V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cbee4f1-8bc0-4416-8369-c0c0e607bdb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job A Hash: Wc8f1Ukh658r79462TQtqWhRD/C281rAjO8S8+QdT08=\n",
            "Job B Hash: Wc8f1Ukh658r79462TQtqWhRD/C281rAjO8S8+QdT08=\n",
            "Job C Hash: 30rEvGn5LdLnRNxy9Y46acaBNzphcdDOpaNex05q5kM=\n",
            "\n",
            "--- Verification ---\n",
            "Hash A and B are identical (as expected): True\n",
            "Hash A and C are different (as expected): True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "9586e3ca",
        "outputId": "6306044d-7b41-4682-e3b7-8015b0080158"
      },
      "source": [
        "# This cell runs the script defined in the previous cell (cell_id: n7mqLs7a82Zz)\n",
        "# and passes a sample URL as an argument.\n",
        "%run /content/drive/MyDrive/Colab Notebooks/INTAKE.py https://vendors.pandologic.com/CollabWORK_A/CollabWORK_A2.xml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "File `'/content/drive/MyDrive/Colab.py'` not found.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_finder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/utils/path.py\u001b[0m in \u001b[0;36mget_py_filename\u001b[0;34m(name, force_win32)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'File `%r` not found.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: File `'/content/drive/MyDrive/Colab.py'` not found.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-1685758108.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This cell runs the script defined in the previous cell (cell_id: n7mqLs7a82Zz)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# and passes a sample URL as an argument.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/INTAKE.py https://vendors.pandologic.com/CollabWORK_A/CollabWORK_A2.xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-52>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nt'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"^'.*'$\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'For Windows, use double quotes to wrap a filename: %run \"mypath\\\\myfile.py\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: File `'/content/drive/MyDrive/Colab.py'` not found."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CHECK:** IS JOB CLOSED?"
      ],
      "metadata": {
        "id": "JQjgK1KUcmuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import base64\n",
        "import hashlib\n",
        "\n",
        "# --- Hashing function updated with learnings from BASE64_JOB.py ---\n",
        "def get_canonical_job_hash(job_data):\n",
        "    \"\"\"\n",
        "    Creates a unique and stable Base64 hash for a job object by focusing\n",
        "    on its core, location-independent content.\n",
        "    \"\"\"\n",
        "    # Core fields that truly determine uniqueness, ignoring IDs and locations.\n",
        "    uniqueness_fields = [\n",
        "        'company_name',\n",
        "        'title',\n",
        "        'description',\n",
        "        'employment_type',\n",
        "    ]\n",
        "    canonical_dict = {key: job_data.get(key) for key in uniqueness_fields}\n",
        "    canonical_string = json.dumps(canonical_dict, sort_keys=True, separators=(',', ':'))\n",
        "    hash_object = hashlib.sha256(canonical_string.encode('utf-8'))\n",
        "    return base64.b64encode(hash_object.digest()).decode('utf-8')\n",
        "\n",
        "# --- Mock Supabase Client for Demonstration ---\n",
        "class MockSupabaseClient:\n",
        "    def __init__(self, initial_data):\n",
        "        self._data = initial_data\n",
        "\n",
        "    def table(self, table_name):\n",
        "        print(f\"[Supabase] Accessing table: '{table_name}'\")\n",
        "        return self\n",
        "\n",
        "    def select(self, columns, filter=None):\n",
        "        print(f\"[Supabase] Selecting '{columns}' from active jobs...\")\n",
        "        # In a real app, this would be: .select(columns).eq(\"status\", \"ACTIVE\")\n",
        "        active_jobs = [item for item in self._data if item.get('status') == 'ACTIVE']\n",
        "        class MockResponse:\n",
        "            def __init__(self, data, columns):\n",
        "                self.data = [{col: item.get(col) for col in columns.split(',')} for item in data]\n",
        "        return MockResponse(active_jobs, columns)\n",
        "\n",
        "    def update(self, new_values):\n",
        "        print(f\"[Supabase] Preparing to update records with: {new_values}\")\n",
        "        return self\n",
        "\n",
        "    def in_(self, column, values):\n",
        "        print(f\"[Supabase] Filtering where '{column}' is in {list(values)}\")\n",
        "        updated_count = 0\n",
        "        for item in self._data:\n",
        "            if item.get(column) in values:\n",
        "                item['status'] = 'CLOSED'\n",
        "                updated_count += 1\n",
        "        print(f\"[Supabase] Mock updated {updated_count} records.\")\n",
        "        return self\n",
        "\n",
        "    def execute(self):\n",
        "        print(\"[Supabase] Execute called. Mock transaction complete.\")\n",
        "        class MockResponse:\n",
        "            data = [{\"status\": \"updated\"}]\n",
        "        return MockResponse()\n",
        "\n",
        "# --- Main Logic ---\n",
        "def check_and_close_jobs(new_feed_jobs, db_client):\n",
        "    \"\"\"\n",
        "    Compares a new job feed against the database to identify and close jobs\n",
        "    that are no longer active.\n",
        "\n",
        "    Args:\n",
        "        new_feed_jobs (list): A list of job dictionaries from the new feed.\n",
        "        db_client: An initialized Supabase client instance.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Job Closure Check ---\")\n",
        "\n",
        "    # 1. Get all job hashes from the new feed using the robust hashing function.\n",
        "    hashes_in_new_feed = {get_canonical_job_hash(job) for job in new_feed_jobs}\n",
        "    print(f\"Found {len(hashes_in_new_feed)} unique job hashes in the new feed.\")\n",
        "\n",
        "    # 2. Get all 'ACTIVE' job hashes currently in the 'open_jobs' table.\n",
        "    response = db_client.table(\"open_jobs\").select(\"id,job_hash\")\n",
        "    active_jobs_in_db = response.data\n",
        "\n",
        "    hashes_in_db = {job['job_hash']: job['id'] for job in active_jobs_in_db}\n",
        "    print(f\"Found {len(hashes_in_db)} active job hashes in the database.\")\n",
        "\n",
        "    # 3. Find which hashes are in the DB but NOT in the new feed.\n",
        "    db_hash_set = set(hashes_in_db.keys())\n",
        "\n",
        "    hashes_to_close = db_hash_set - hashes_in_new_feed\n",
        "\n",
        "    if not hashes_to_close:\n",
        "        print(\"No jobs to close. All active jobs in DB are present in the new feed.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(hashes_to_close)} jobs to mark as CLOSED.\")\n",
        "    print(\"Hashes to close:\", hashes_to_close)\n",
        "\n",
        "    # 4. Update the status of these jobs to 'CLOSED' in the database.\n",
        "    db_client.table(\"open_jobs\").update({\"status\": \"CLOSED\"}).in_(\"job_hash\", list(hashes_to_close)).execute()\n",
        "\n",
        "    print(\"--- Job Closure Check Complete ---\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Setup More Realistic Mock Data ---\n",
        "    job_engineer_sf = {\n",
        "        \"company_name\": \"Innovatech\", \"title\": \"Software Engineer\",\n",
        "        \"description\": \"Build cool stuff.\", \"employment_type\": \"FULL_TIME\",\n",
        "        \"external_job_id\": \"111\", \"locations\": [{\"city\": \"San Francisco\"}]\n",
        "    }\n",
        "\n",
        "    job_engineer_ny = { # Same core job as above, different location/id\n",
        "        \"company_name\": \"Innovatech\", \"title\": \"Software Engineer\",\n",
        "        \"description\": \"Build cool stuff.\", \"employment_type\": \"FULL_TIME\",\n",
        "        \"external_job_id\": \"222\", \"locations\": [{\"city\": \"New York\"}]\n",
        "    }\n",
        "\n",
        "    job_analyst = { # This job will be closed\n",
        "        \"company_name\": \"Data Corp\", \"title\": \"Data Analyst\",\n",
        "        \"description\": \"Analyze data.\", \"employment_type\": \"FULL_TIME\",\n",
        "        \"external_job_id\": \"333\"\n",
        "    }\n",
        "\n",
        "    # Generate the hashes\n",
        "    hash_engineer = get_canonical_job_hash(job_engineer_sf)\n",
        "    hash_analyst = get_canonical_job_hash(job_analyst)\n",
        "\n",
        "    print(f\"Engineer Hash: {hash_engineer}\")\n",
        "    print(f\"Analyst Hash:  {hash_analyst}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    # Simulate the current state of the 'open_jobs' table in Supabase\n",
        "    mock_db_data = [\n",
        "        {\"id\": 1, \"job_hash\": hash_engineer, \"status\": \"ACTIVE\"},\n",
        "        {\"id\": 2, \"job_hash\": hash_analyst, \"status\": \"ACTIVE\"},\n",
        "    ]\n",
        "\n",
        "    # Simulate a new job feed from a source. The Analyst job is missing,\n",
        "    # and the Engineer job is present, but with a different location/id.\n",
        "    new_feed = [job_engineer_ny]\n",
        "\n",
        "    # Initialize the mock client with our data\n",
        "    mock_supabase_client = MockSupabaseClient(mock_db_data)\n",
        "\n",
        "    # Run the process\n",
        "    check_and_close_jobs(new_feed, mock_supabase_client)\n",
        "\n"
      ],
      "metadata": {
        "id": "UAzNS9bZcmRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CHECK:** IS JOB A DUPLICATE?"
      ],
      "metadata": {
        "id": "XHyIppL4cv5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import base64\n",
        "import hashlib\n",
        "\n",
        "# --- Hashing function updated with new learnings ---\n",
        "def get_canonical_job_hash(job_data):\n",
        "    \"\"\"\n",
        "    Creates a unique and stable Base64 hash for a job object by focusing\n",
        "    on its core, location-independent content.\n",
        "    \"\"\"\n",
        "    # Core fields that truly determine uniqueness, ignoring IDs and locations.\n",
        "    uniqueness_fields = [\n",
        "        'company_name',\n",
        "        'title',\n",
        "        'description',\n",
        "        'employment_type',\n",
        "    ]\n",
        "    canonical_dict = {key: job_data.get(key) for key in uniqueness_fields}\n",
        "    canonical_string = json.dumps(canonical_dict, sort_keys=True, separators=(',', ':'))\n",
        "    hash_object = hashlib.sha256(canonical_string.encode('utf-8'))\n",
        "    return base64.b64encode(hash_object.digest()).decode('utf-8')\n",
        "\n",
        "# --- Mock Supabase Client for Demonstration ---\n",
        "class MockSupabaseClient:\n",
        "    def __init__(self, initial_data):\n",
        "        self._data = initial_data\n",
        "        self._hashes = {item['job_hash'] for item in initial_data}\n",
        "\n",
        "    def table(self, table_name):\n",
        "        print(f\"[Supabase] Accessing table: '{table_name}'\")\n",
        "        return self\n",
        "\n",
        "    def insert(self, data):\n",
        "        print(f\"[Supabase] Inserting data...\")\n",
        "        print(json.dumps(data, indent=2))\n",
        "        self._data.append(data)\n",
        "        self._hashes.add(data['job_hash'])\n",
        "        class MockResponse:\n",
        "            data = [{\"status\": \"inserted\"}]\n",
        "        return MockResponse()\n",
        "\n",
        "    def get_existing_hashes(self):\n",
        "        print(\"[Supabase] Fetching all existing job hashes...\")\n",
        "        return self._hashes\n",
        "\n",
        "# --- Main Logic ---\n",
        "def process_and_insert_jobs(new_jobs, db_client):\n",
        "    \"\"\"\n",
        "    Processes a list of new jobs, checks for duplicates using their hash,\n",
        "    and inserts only the unique ones.\n",
        "\n",
        "    Args:\n",
        "        new_jobs (list): A list of new job dictionaries to process.\n",
        "        db_client: An initialized Supabase client instance.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Duplicate Job Check and Insertion ---\")\n",
        "\n",
        "    # 1. For efficiency, get all existing job hashes from the database in one query.\n",
        "    existing_hashes = db_client.get_existing_hashes()\n",
        "    print(f\"Found {len(existing_hashes)} existing job hashes in the database.\")\n",
        "\n",
        "    # 2. Iterate through new jobs, generate their hash, and check for existence.\n",
        "    new_jobs_inserted = 0\n",
        "    for job in new_jobs:\n",
        "        # Generate the unique hash for the incoming job using the robust method\n",
        "        job_hash = get_canonical_job_hash(job)\n",
        "\n",
        "        print(f\"\\nProcessing job '{job.get('title')}' | Hash: {job_hash[:10]}...\")\n",
        "\n",
        "        if job_hash in existing_hashes:\n",
        "            print(\"Result: DUPLICATE. Job already exists in the database. Skipping.\")\n",
        "        else:\n",
        "            print(\"Result: UNIQUE. Inserting new job.\")\n",
        "            # Add the hash to the job data before insertion\n",
        "            job_to_insert = job.copy()\n",
        "            job_to_insert['job_hash'] = job_hash\n",
        "\n",
        "            # Insert the new job\n",
        "            db_client.table(\"open_jobs\").insert(job_to_insert)\n",
        "\n",
        "            # Add the new hash to our set to prevent duplicate insertions from the same feed\n",
        "            existing_hashes.add(job_hash)\n",
        "            new_jobs_inserted += 1\n",
        "\n",
        "    print(f\"\\n--- Process Complete. Inserted {new_jobs_inserted} new jobs. ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Setup More Realistic Mock Data ---\n",
        "    # This is the job that already exists in our database.\n",
        "    existing_engineer_job = {\n",
        "        \"company_name\": \"Innovatech\", \"title\": \"Software Engineer\",\n",
        "        \"description\": \"Build cool stuff.\", \"employment_type\": \"FULL_TIME\",\n",
        "        \"external_job_id\": \"111\", \"locations\": [{\"city\": \"San Francisco\"}]\n",
        "    }\n",
        "\n",
        "    # This job is a DUPLICATE of the one above, but from a different feed\n",
        "    # with a different ID and location. It should be skipped.\n",
        "    duplicate_engineer_job_from_feed = {\n",
        "        \"company_name\": \"Innovatech\", \"title\": \"Software Engineer\",\n",
        "        \"description\": \"Build cool stuff.\", \"employment_type\": \"FULL_TIME\",\n",
        "        \"external_job_id\": \"abc-987\", \"locations\": [{\"city\": \"New York\"}]\n",
        "    }\n",
        "\n",
        "    # This is a genuinely new job that should be inserted.\n",
        "    new_analyst_job = {\n",
        "        \"company_name\": \"Data Corp\", \"title\": \"Data Analyst\",\n",
        "        \"description\": \"Analyze data.\", \"employment_type\": \"FULL_TIME\",\n",
        "        \"external_job_id\": \"333\"\n",
        "    }\n",
        "\n",
        "    # Generate the hash for the existing job\n",
        "    existing_hash = get_canonical_job_hash(existing_engineer_job)\n",
        "\n",
        "    # Simulate the current state of the 'open_jobs' table in Supabase\n",
        "    mock_db_data = [\n",
        "        {\"id\": 1, \"job_hash\": existing_hash, \"status\": \"ACTIVE\", **existing_engineer_job},\n",
        "    ]\n",
        "\n",
        "    # Simulate a new job feed containing the duplicate and the new job\n",
        "    new_feed = [duplicate_engineer_job_from_feed, new_analyst_job]\n",
        "\n",
        "    # Initialize the mock client with our data\n",
        "    mock_supabase_client = MockSupabaseClient(mock_db_data)\n",
        "\n",
        "    # Run the process\n",
        "    process_and_insert_jobs(new_feed, mock_supabase_client)\n"
      ],
      "metadata": {
        "id": "tS5OxWzWc0H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AI ENRICHMENT:** ENHANCE JOB DATA SCHEMA WITH AI_ FIELDS"
      ],
      "metadata": {
        "id": "NKT_wC4Vc4mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import asyncio\n",
        "\n",
        "# --- Mock Gemini API Client ---\n",
        "# This mock simulates the behavior of calling the Gemini API.\n",
        "# In a real application, you would replace this with actual calls\n",
        "# using a library like `google-generativeai`.\n",
        "\n",
        "class MockGeminiClient:\n",
        "    async def generate_content_async(self, prompt):\n",
        "        print(\"\\n--- Calling Mock Gemini API ---\")\n",
        "        print(f\"Prompt: {prompt[:100]}...\")\n",
        "        # Simulate network latency\n",
        "        await asyncio.sleep(1)\n",
        "\n",
        "        # Simulate response for industry classification\n",
        "        if \"sector, industry_group, and industry\" in prompt:\n",
        "            mock_response_text = json.dumps({\n",
        "                \"sector\": \"Technology\",\n",
        "                \"industry_group\": \"Software & IT Services\",\n",
        "                \"industry\": \"Software\",\n",
        "                \"industry_id\": 501\n",
        "            })\n",
        "            print(\"Response Type: Industry Classification\")\n",
        "            return type('obj', (object,), {'text': mock_response_text}) # Return a mock response object with a .text attribute\n",
        "\n",
        "\n",
        "        # Simulate response for AI attribute generation\n",
        "        if \"ai_title, ai_description\" in prompt:\n",
        "            mock_response_text = json.dumps({\n",
        "                \"ai_title\": \"Senior Backend Software Engineer\",\n",
        "                \"ai_description\": \"Join a dynamic platform team to design, develop, and maintain scalable microservices using Go and AWS. Focus on building robust backend systems and improving CI/CD pipelines.\",\n",
        "                \"ai_job_tasks\": [\n",
        "                    \"Design and develop backend microservices in Go.\",\n",
        "                    \"Maintain and improve CI/CD pipelines on AWS.\",\n",
        "                    \"Collaborate with cross-functional teams.\",\n",
        "                    \"Mentor junior engineers and conduct code reviews.\"\n",
        "                ],\n",
        "                \"ai_search_terms\": [\"golang\", \"go\", \"aws\", \"kubernetes\", \"docker\", \"backend engineer\", \"microservices\"],\n",
        "                \"ai_top_tags\": [\"Go\", \"AWS\", \"Backend\"],\n",
        "                \"ai_job_function_id\": 105,\n",
        "                \"ai_skills\": [\"Go (Golang)\", \"Amazon Web Services (AWS)\", \"Docker\", \"Kubernetes\", \"Microservices Architecture\", \"CI/CD\", \"Problem Solving\"],\n",
        "                \"ai_confidence_score\": 0.95 # Added confidence score\n",
        "            })\n",
        "            print(\"Response Type: AI Attribute Generation\")\n",
        "            return type('obj', (object,), {'text': mock_response_text}) # Return a mock response object with a .text attribute\n",
        "\n",
        "\n",
        "        return type('obj', (object,), {'text': \"{}\"}) # Return a mock response object with a .text attribute\n",
        "\n",
        "\n",
        "# --- Gemini Service Class ---\n",
        "\n",
        "class GeminiService:\n",
        "    def __init__(self, gemini_client):\n",
        "        \"\"\"\n",
        "        Initializes the GeminiService with a Gemini API client.\n",
        "\n",
        "        Args:\n",
        "            gemini_client: An instance of a Gemini API client (e.g., MockGeminiClient or google.generativeai.GenerativeModel).\n",
        "        \"\"\"\n",
        "        self._client = gemini_client\n",
        "        self._industry_cache = {} # Cache for industry classifications\n",
        "        self._industry_batch = [] # Batch for industry classification calls\n",
        "        self._batch_limit = 5 # Process every 5 jobs or when explicitly called\n",
        "\n",
        "    def add_job_for_industry_classification(self, job):\n",
        "        \"\"\"Adds a job to the batch for industry classification.\"\"\"\n",
        "        job_key = (job.get('title'), job.get('description'))\n",
        "        if job_key in self._industry_cache:\n",
        "            print(f\"Industry cache hit for job title: {job.get('title')}\")\n",
        "            return self._industry_cache[job_key]\n",
        "\n",
        "        self._industry_batch.append(job)\n",
        "        print(f\"Added job '{job.get('title')}' to industry batch. Current batch size: {len(self._industry_batch)}\")\n",
        "\n",
        "    async def process_industry_batch(self):\n",
        "        \"\"\"Processes the current batch of jobs for industry classification.\"\"\"\n",
        "        if not self._industry_batch:\n",
        "            print(\"Industry batch is empty. Nothing to process.\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\n--- Processing an industry batch of {len(self._industry_batch)} jobs ---\")\n",
        "\n",
        "        # Create a single prompt for the entire batch\n",
        "        prompt_lines = [\"Analyze the following job listings (title and description) and return a JSON object for each with its sector, industry_group, and industry id.\", \"---\"]\n",
        "        for i, job in enumerate(self._industry_batch):\n",
        "            prompt_lines.append(f\"Job {i+1}:\")\n",
        "            prompt_lines.append(f\"Title: {job.get('title')}\")\n",
        "            prompt_lines.append(f\"Description: {job.get('description')}\")\n",
        "            prompt_lines.append(\"---\")\n",
        "\n",
        "        prompt = \"\\n\".join(prompt_lines)\n",
        "\n",
        "        # In a real scenario, the response would contain a list of classifications.\n",
        "        # Here, we simulate it by applying the same mock response to all batch items.\n",
        "        response = await self._client.generate_content_async(prompt)\n",
        "        classification = json.loads(response.text) # Assuming response has a .text attribute\n",
        "\n",
        "        for job in self._industry_batch:\n",
        "            job.update(classification)\n",
        "            job_key = (job.get('title'), job.get('description'))\n",
        "            self._industry_cache[job_key] = classification # Cache the result\n",
        "            print(f\"Enriched job '{job.get('title')}' with industry info.\")\n",
        "\n",
        "        # Clear the batch after processing\n",
        "        self._industry_batch.clear()\n",
        "        print(\"Industry batch processing complete.\")\n",
        "\n",
        "    async def generate_ai_attributes(self, job_data):\n",
        "        \"\"\"\n",
        "        Generates AI-powered attributes for a single job posting.\n",
        "\n",
        "        Args:\n",
        "            job_data (dict): The job dictionary, ideally already enriched with industry info.\n",
        "\n",
        "        Returns:\n",
        "            dict: The job dictionary updated with ai_ prefixed fields.\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- Generating AI attributes for job: {job_data.get('title')} ---\")\n",
        "\n",
        "        # Create a detailed prompt for Gemini\n",
        "        prompt = f\"\"\"\n",
        "        Based on the following job data, generate a structured JSON object containing these fields:\n",
        "        ai_title, ai_description, ai_job_tasks, ai_search_terms, ai_top_tags, ai_job_function_id, ai_skills, ai_confidence_score.\n",
        "        The confidence score should be a float between 0.0 and 1.0.\n",
        "\n",
        "        Job Data:\n",
        "        - Title: {job_data.get('title')}\n",
        "        - Company: {job_data.get('company_name')}\n",
        "        - Description: {job_data.get('description')}\n",
        "        - Industry: {job_data.get('industry', 'N/A')}\n",
        "\n",
        "        Generate the response in a clean JSON format.\n",
        "        \"\"\"\n",
        "\n",
        "        response = await self._client.generate_content_async(prompt)\n",
        "        ai_data = json.loads(response.text) # Assuming response has a .text attribute\n",
        "\n",
        "        job_data.update(ai_data)\n",
        "        print(\"Successfully enriched job with AI attributes.\")\n",
        "        return job_data\n",
        "\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "\n",
        "async def main():\n",
        "    # A list of new, unique jobs ready for enrichment\n",
        "    new_jobs = [\n",
        "        {\n",
        "            \"external_job_id\": \"gh_1a2b3c4d\",\n",
        "            \"company_name\": \"Innovatech Solutions Inc.\",\n",
        "            \"title\": \"Senior Software Engineer (Backend)\",\n",
        "            \"description\": \"Join our platform team to build scalable microservices using Go and AWS.\"\n",
        "        },\n",
        "        { # A different job to show batching\n",
        "            \"external_job_id\": \"ln_5f6g7h8i\",\n",
        "            \"company_name\": \"HealthData Corp\",\n",
        "            \"title\": \"Clinical Data Analyst\",\n",
        "            \"description\": \"Analyze clinical trial data using Python and SQL.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Initialize the mock client and the Gemini Service\n",
        "    mock_gemini_client = MockGeminiClient()\n",
        "    gemini_service = GeminiService(mock_gemini_client)\n",
        "\n",
        "    # 1. Classify industries in a batch\n",
        "    for job in new_jobs:\n",
        "        gemini_service.add_job_for_industry_classification(job)\n",
        "\n",
        "    await gemini_service.process_industry_batch() # Process all jobs collected so far\n",
        "\n",
        "    # 2. Generate AI attributes for each enriched job\n",
        "    enriched_jobs = []\n",
        "    for job in new_jobs:\n",
        "        fully_enriched_job = await gemini_service.generate_ai_attributes(job)\n",
        "        enriched_jobs.append(fully_enriched_job)\n",
        "\n",
        "    # 3. Display the final, fully enriched job data\n",
        "    print(\"\\n\\n--- FINAL ENRICHED JOB DATA ---\")\n",
        "    print(json.dumps(enriched_jobs, indent=2))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "tY1Nr29MdA2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AI CONFIDENCE ASSESSMENT:** MEASURE THE SCORE"
      ],
      "metadata": {
        "id": "lOEyDWGefIsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# In a real application, you would import these functions from their respective files.\n",
        "# from AUTO_APPROVE import sync_to_xano\n",
        "# from MANUAL_REVIEW import send_for_manual_review\n",
        "\n",
        "# For demonstration, we'll include mock versions of the functions here.\n",
        "def sync_to_xano(job_data):\n",
        "    \"\"\"Mock function to simulate syncing a job to a Xano database.\"\"\"\n",
        "    print(f\"-> Action: Syncing job '{job_data.get('ai_title')}' to Xano database.\")\n",
        "    # In a real script, this would involve an API call to Xano.\n",
        "    print(\"-> Status: AUTO-APPROVED\")\n",
        "\n",
        "def send_for_manual_review(job_data):\n",
        "    \"\"\"Mock function to simulate sending a job for manual review.\"\"\"\n",
        "    print(f\"-> Action: Sending job '{job_data.get('ai_title')}' for manual review.\")\n",
        "    # This could save the job to a different table, a file, or send a notification.\n",
        "    print(\"-> Status: PENDING MANUAL REVIEW\")\n",
        "\n",
        "\n",
        "def check_confidence_and_route(job, confidence_threshold=0.86):\n",
        "    \"\"\"\n",
        "    Checks the AI confidence score of a job and routes it for automatic\n",
        "    approval or manual review.\n",
        "\n",
        "    Args:\n",
        "        job (dict): The enriched job data dictionary.\n",
        "        confidence_threshold (float): The score above which jobs are auto-approved.\n",
        "    \"\"\"\n",
        "    job_title = job.get('ai_title', job.get('title', 'N/A'))\n",
        "    confidence_score = job.get('ai_confidence_score')\n",
        "\n",
        "    print(f\"--- Processing Job: '{job_title}' ---\")\n",
        "\n",
        "    if confidence_score is None:\n",
        "        print(f\"Confidence score not found. Defaulting to manual review.\")\n",
        "        send_for_manual_review(job)\n",
        "        return\n",
        "\n",
        "    print(f\"Confidence Score: {confidence_score} | Threshold: {confidence_threshold}\")\n",
        "\n",
        "    if confidence_score >= confidence_threshold:\n",
        "        print(\"Confidence is above or equal to threshold.\")\n",
        "        sync_to_xano(job)\n",
        "    else:\n",
        "        print(\"Confidence is below threshold.\")\n",
        "        send_for_manual_review(job)\n",
        "\n",
        "# --- Main execution block to demonstrate the routing logic ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Example of a high-confidence job that should be auto-approved\n",
        "    high_confidence_job = {\n",
        "        \"ai_title\": \"Senior Backend Software Engineer\",\n",
        "        \"ai_description\": \"Join a dynamic platform team...\",\n",
        "        \"ai_skills\": [\"Go (Golang)\", \"AWS\", \"Docker\"],\n",
        "        \"ai_confidence_score\": 0.95\n",
        "    }\n",
        "\n",
        "    # Example of a low-confidence job that needs manual review\n",
        "    low_confidence_job = {\n",
        "        \"ai_title\": \"Product Marketing Associate (Entry Level)\",\n",
        "        \"ai_description\": \"Seeking a creative individual for marketing tasks.\",\n",
        "        \"ai_skills\": [\"Marketing\", \"Social Media\"],\n",
        "        \"ai_confidence_score\": 0.78\n",
        "    }\n",
        "\n",
        "    # Example of a job with a missing confidence score\n",
        "    no_score_job = {\n",
        "        \"ai_title\": \"IT Helpdesk Technician\",\n",
        "        \"ai_description\": \"Provide technical support to internal employees.\",\n",
        "        \"ai_skills\": [\"Active Directory\", \"Troubleshooting\"]\n",
        "        # No ai_confidence_score field\n",
        "    }\n",
        "\n",
        "    print(\"--- Starting AI Confidence Check Workflow ---\\n\")\n",
        "    check_confidence_and_route(high_confidence_job)\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "    check_confidence_and_route(low_confidence_job)\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "    check_confidence_and_route(no_score_job)\n",
        "    print(\"\\n--- Workflow Complete ---\")\n",
        "\n"
      ],
      "metadata": {
        "id": "UXCWbk1ofQFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AUTO-APPROVAL:** IF ABOVE AI CONFIDENCE SCORE THRESHOLD"
      ],
      "metadata": {
        "id": "qRxIAE7kfQ5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# --- Mock Xano Client for Demonstration ---\n",
        "# In a real application, you would use a library like 'requests'\n",
        "# to make API calls to your Xano instance.\n",
        "class MockXanoClient:\n",
        "    def __init__(self, api_url, api_key):\n",
        "        self.api_url = api_url\n",
        "        self.api_key = api_key\n",
        "        print(f\"[Xano Client] Initialized for endpoint: {api_url}\")\n",
        "\n",
        "    def post(self, endpoint, data):\n",
        "        \"\"\"Simulates posting data to a Xano endpoint.\"\"\"\n",
        "        print(f\"[Xano Client] POST to '{endpoint}'\")\n",
        "        print(\"[Xano Client] Data Payload:\")\n",
        "        print(json.dumps(data, indent=2))\n",
        "        # Simulate a successful API response from Xano\n",
        "        return {\"status\": \"success\", \"record_id\": 12345}\n",
        "\n",
        "# In a real app, you'd get these from environment variables\n",
        "XANO_API_URL = \"https://xano.example.com/api:your_instance\"\n",
        "XANO_API_KEY = \"YOUR_XANO_API_KEY\"\n",
        "\n",
        "# Initialize the client\n",
        "xano_client = MockXanoClient(XANO_API_URL, XANO_API_KEY)\n",
        "\n",
        "def sync_to_xano(job_data):\n",
        "    \"\"\"\n",
        "    Takes a validated and approved job object and syncs it to a Xano database.\n",
        "\n",
        "    Args:\n",
        "        job_data (dict): The job data to be uploaded.\n",
        "    \"\"\"\n",
        "    print(f\"--- Auto-Approving and Syncing Job: '{job_data.get('ai_title')}' ---\")\n",
        "\n",
        "    # You might want to remove the confidence score before syncing\n",
        "    if 'ai_confidence_score' in job_data:\n",
        "        del job_data['ai_confidence_score']\n",
        "\n",
        "    try:\n",
        "        # The endpoint for your jobs table in Xano\n",
        "        jobs_endpoint = \"/open_jobs\"\n",
        "        response = xano_client.post(jobs_endpoint, data=job_data)\n",
        "\n",
        "        if response.get(\"status\") == \"success\":\n",
        "            print(f\"Successfully synced job to Xano. New Record ID: {response.get('record_id')}\")\n",
        "        else:\n",
        "            print(\"Error: Failed to sync job to Xano.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during Xano sync: {e}\")\n",
        "\n",
        "# --- Main execution block to demonstrate standalone usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Example of a job that has passed the confidence check\n",
        "    approved_job = {\n",
        "        \"ai_title\": \"Senior Backend Software Engineer\",\n",
        "        \"ai_description\": \"Join a dynamic platform team...\",\n",
        "        \"ai_skills\": [\"Go (Golang)\", \"AWS\", \"Docker\"],\n",
        "        \"ai_confidence_score\": 0.95 # This will be removed by the function\n",
        "    }\n",
        "\n",
        "    sync_to_xano(approved_job)\n"
      ],
      "metadata": {
        "id": "WzD3OY3QfY16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MANUAL REVIEW:** IF BELOW AI THRESHOLD SCORE"
      ],
      "metadata": {
        "id": "gdjuOJ_efcp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Define the file where jobs pending review will be stored\n",
        "MANUAL_REVIEW_QUEUE_FILE = \"manual_review_queue.json\"\n",
        "\n",
        "def load_review_queue():\n",
        "    \"\"\"Loads the existing review queue from the JSON file.\"\"\"\n",
        "    if os.path.exists(MANUAL_REVIEW_QUEUE_FILE):\n",
        "        with open(MANUAL_REVIEW_QUEUE_FILE, 'r') as f:\n",
        "            try:\n",
        "                return json.load(f)\n",
        "            except json.JSONDecodeError:\n",
        "                return []\n",
        "    return []\n",
        "\n",
        "def save_review_queue(queue_data):\n",
        "    \"\"\"Saves the updated review queue to the JSON file.\"\"\"\n",
        "    with open(MANUAL_REVIEW_QUEUE_FILE, 'w') as f:\n",
        "        json.dump(queue_data, f, indent=2)\n",
        "\n",
        "def send_for_manual_review(job_data):\n",
        "    \"\"\"\n",
        "    Takes a job that is below the confidence threshold and adds it to a\n",
        "    manual review queue.\n",
        "\n",
        "    Args:\n",
        "        job_data (dict): The job data to be reviewed.\n",
        "    \"\"\"\n",
        "    title = job_data.get('ai_title', 'N/A')\n",
        "    print(f\"--- Sending Job for Manual Review: '{title}' ---\")\n",
        "\n",
        "    # Load the current queue\n",
        "    review_queue = load_review_queue()\n",
        "\n",
        "    # Add metadata for the review process\n",
        "    review_item = {\n",
        "        \"review_status\": \"pending\",\n",
        "        \"added_to_queue_at\": datetime.utcnow().isoformat() + 'Z',\n",
        "        \"job_data\": job_data\n",
        "    }\n",
        "\n",
        "    # Add the new item to the queue\n",
        "    review_queue.append(review_item)\n",
        "\n",
        "    # Save the updated queue\n",
        "    save_review_queue(review_queue)\n",
        "\n",
        "    print(f\"Job successfully added to '{MANUAL_REVIEW_QUEUE_FILE}'.\")\n",
        "    print(f\"Total jobs in queue: {len(review_queue)}\")\n",
        "\n",
        "# --- Main execution block to demonstrate standalone usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Example of a job that failed the confidence check\n",
        "    job_to_review = {\n",
        "        \"ai_title\": \"Product Marketing Associate (Entry Level)\",\n",
        "        \"ai_description\": \"Seeking a creative individual for marketing tasks.\",\n",
        "        \"ai_skills\": [\"Marketing\", \"Social Media\"],\n",
        "        \"ai_confidence_score\": 0.78\n",
        "    }\n",
        "\n",
        "    send_for_manual_review(job_to_review)\n",
        "\n",
        "    # You can run this script multiple times to see the queue grow.\n",
        "    # To see the contents, open the 'manual_review_queue.json' file\n",
        "    # that will be created in the same directory.\n"
      ],
      "metadata": {
        "id": "DOE02jHFfhXJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}